import torch
import torch.nn as nn
import torch.nn.functional as f


# 将算法整个前向过程合成一个大网络
class HamsModel(nn.Module):
    def __init__(self, args, slaves, masters, gcms, commgroup, critic, eval_hidden, h_m, c_m, h_comm, c_comm):
        super(HamsModel, self).__init__()
        self.n_slaves = args.n_slaves
        self.n_master = args.n_master
        self.args = args

        #  cluster network
        self.slaves = slaves
        self.masters = masters
        self.commGroup = commgroup
        self.gcms = gcms
        self.critic = critic

        # hidden state
        self.eval_hidden = eval_hidden
        self.h_master = h_m
        self.c_master = c_m
        self.h_commGroup = h_comm
        self.c_commGroup = c_comm

    def forward(self, batch, max_episode_len, epsilon):
        """
        计算所有智能体动作概率和估计v值。
        batch为采样得到的全部样本：(episode_num, episode_len, n_agents, shape)
        max_episode_len为每个episode的最大长度
        epsilon为探索概率
        """
        episode_num = batch["o"].shape[0]  # batch_size，即batch中episode数量
        avail_actions = batch["avail_u"]  # (batch_size, episode_len, n_agent, n_action)
        action_prob = []  # 智能体采取的动作的概率
        v_evals = []  # 价值网络的v值估计
        for transition_idx in range(max_episode_len):
            # ***********************Policy network***********************
            # Slave:获取各集群slave的输入
            # 给obs加last_action、agent_id，列表元素维度(batch_size * n_slave, input_shape)
            slave_inputs = self._get_slave_inputs(batch, transition_idx)
            if self.args.cuda:
                for i in range(self.n_master):
                    slave_inputs[i] = slave_inputs[i].cuda()
                    self.eval_hidden[i] = self.eval_hidden[i].cuda()
                    self.h_master[i] = self.h_master[i].cuda()
                    self.c_master[i] = self.c_master[i].cuda()
                self.h_commGroup = self.h_commGroup.cuda()
                self.c_commGroup = self.c_commGroup.cuda()
            # 将slave自身观测、隐藏状态输入得到slave输出和当前时刻的隐藏状态
            # 输出第一个为智能体自身策略，第二个为h_slave，第三个为加入其他信息的h_slave'
            action_prob_ss, new_h_slave = [], []
            for i in range(len(self.n_slaves)):
                # action_prob_i: (batch_size * n_slave, n_actions)
                # self.eval_hidden[i]: (batch_size * n_slave, slave_hidden_dim)
                # new_h_slave_i: (batch_size * n_slave, slave_hidden_dim + attention_dim)
                action_prob_i, self.eval_hidden[i], new_h_slave_i = self.slaves[i](slave_inputs[i],
                                                                                   self.eval_hidden[i],
                                                                                   self.n_slaves[i])
                # 变换维度并保存 (batch_size, n_slave, n_actions)
                action_prob_ss.append(action_prob_i.view(episode_num, self.n_slaves[i], -1))
                new_h_slave.append(new_h_slave_i)

            # Master:
            # master_states维度(batch_size, n_master, state_shape)
            # h_s为列表每个元素维度(batch_size, n_slave, slave_hidden_dim + attention_dim)
            master_states, h_s = self._get_master_inputs(batch, transition_idx, new_h_slave)
            for i in range(self.n_master):
                # self.h_master[i]: (batch_size, 1, master_hidden_dim)
                # self.c_master[i]: (batch_size, 1, master_hidden_dim)
                # 由全局状态和h_s得到master的h_m和c_m，后续还要经过commGroup、GCM两部分才能得到master的动作策略和其对slave的指导策略
                self.h_master[i], self.c_master[i] = self.masters[i](master_states[:, i],
                                                                     self.h_master[i],
                                                                     self.c_master[i],
                                                                     h_s[i])

            # CommGroup:
            # 将h_m进行拼接(batch_size, n_master, master_hidden_dim)
            comm_inputs = torch.stack(self.h_master, dim=1)
            # (batch_size * n_master, master_hidden_dim)
            comm_inputs = comm_inputs.reshape(episode_num * self.n_master, -1)
            _, self.h_commGroup, self.c_commGroup = self.commGroup(comm_inputs, self.h_commGroup, self.c_commGroup)
            # (batch_size, n_master, gcm_hidden_dim)
            self.h_commGroup = self.h_commGroup.view(episode_num, self.n_master, -1)
            # (batch_size, n_master, gcm_hidden_dim)
            self.c_commGroup = self.c_commGroup.view(episode_num, self.n_master, -1)

            # GCM:
            # 每个master的h_m'和c_m'送入GCM计算对组内slave的指导动作策略
            action_prob_ms = []
            for i in range(self.n_master):
                # (batch_size * n_slave, n_actions)
                action_prob_j = self.gcms[i](self.h_commGroup[:, i],
                                             self.c_commGroup[:, i],
                                             new_h_slave[i],
                                             self.n_slaves[i])
                action_prob_ms.append(action_prob_j.view(episode_num, self.n_slaves[i], -1))

            # Merge the slave policy and master policy
            s_actions = []
            for i in range(len(self.n_slaves)):
                # element dim: (batch_size, n_slave, n_actions)
                s_actions.append(action_prob_ss[i] + action_prob_ms[i])

            # 将各集群智能体动作策略拼接起来(batch_size, n_agents, n_actions)
            output = torch.cat(s_actions, dim=1)
            prob = torch.nn.functional.softmax(output, dim=-1)  # 输出经过softmax归一化
            action_prob.append(prob)  # 在当前时间步所有智能体的动作概率

            # ***********************Value network***********************
            critic_inputs = batch["s"][:, transition_idx]
            if self.args.cuda:
                critic_inputs = critic_inputs.cuda()
            # value network input dim: (batch_size, state_shape), out dim: (batch_size, 1)
            v_eval = self.critic(critic_inputs)
            v_evals.append(v_eval)

        # stack at the timestep, dim: (batch_size, max_episode_len, 1)
        v_evals = torch.stack(v_evals, dim=1)

        # 得到的action_prob是一个列表，列表里装着max_episode_len个数组，数组每个元素的维度(batch_size, n_agents, n_actions)
        # 把该列表转化成(batch_size, max_episode_len, n_agents, n_actions)的数组
        action_prob = torch.stack(action_prob, dim=1).cpu()
        # 可选动作的个数 (batch_size, max_episode_len, n_agents, n_actions)
        action_num = avail_actions.sum(dim=-1, keepdim=True).float().repeat(1, 1, 1, avail_actions.shape[-1])
        # 选动作时增加探索
        action_prob = ((1 - epsilon) * action_prob + torch.ones_like(action_prob) * epsilon / action_num)
        action_prob[avail_actions == 0] = 0.0  # 不能执行的动作置0
        # 上一步将不能执行动作置零，概率和不为1，需要重新正则化，执行过程中Categorical会自己正则化
        action_prob = action_prob / action_prob.sum(dim=-1, keepdim=True)
        # 因为有许多经验是填充的，它们的avail_actions都填充的是0，所以该经验上所有动作的概率都为0，在正则化的时候会得到nan
        # 因此需要再一次将该经验对应的概率置为0
        action_prob[avail_actions == 0] = 0.0
        if self.args.cuda:
            action_prob = action_prob.cuda()
            v_evals = v_evals.cuda()

        return action_prob, v_evals

    def _get_slave_inputs(self, batch, transition_idx):
        """
        计算所有集群的slave输入。
        batch为所有集群的输入，计算时需要对集群进行区分，取出对应的样本信息进行计算。
        """
        # 取出所有episode上该transition_idx的经验
        # obs与u_onehot均为列表，obs内元素维度(batch_size, n_slave, obs_shape)
        # u_onehot内元素维度(batch_size, max_episode_len, n_slave, n_actions)
        obs, u_onehot = [], []
        index = 0
        # 取出对应的样本
        for i in range(len(self.n_slaves)):
            group_agents = self.n_slaves[i]
            index += group_agents
            obs.append(batch["o"][:, transition_idx, index - group_agents:index])
            u_onehot.append(batch["u_onehot"][:, :, index - group_agents:index])

        episode_num = batch["o"].shape[0]
        inputs = []  # 所有集群slave的输入，是一个列表，每个元素维度(batch_size * n_slave, obs_shape + n_action + n_slave)
        for i in range(len(self.n_slaves)):
            input_i = list()
            input_i.append(obs[i])  # 集群i内slave的输入，给input_i加上一个动作、agent编号
            if self.args.last_action:
                if transition_idx == 0:  # 如果是第一条经验，则就让前一个动作为零向量
                    input_i.append(torch.zeros_like(u_onehot[i][:, transition_idx]))
                else:
                    input_i.append(u_onehot[i][:, transition_idx - 1])
            if self.args.reuse_network:
                '''
                因为当前的inputs三维的数据，每一维分别代表(episode编号, agent编号, inputs维度)，直接在dim=1上添加对应的向量即可
                比如给agent_0后面加(1, 0, 0, 0, 0), 表示5个agent中的0号，而agent_0的数据正好在第0行，那么需要加的agent编号恰好
                就是一个单位阵，即对角线为1，其余为0
                '''
                input_i.append(torch.eye(self.n_slaves[i]).unsqueeze(0).expand(episode_num, -1, -1))
            # 要把inputs中的三个拼起来，因为群内slave共享一个网络，每条数据中带上了自己的编号，所以还是自己的数据
            input_i = torch.cat([x.reshape(episode_num * self.n_slaves[i], -1) for x in input_i], dim=1)
            inputs.append(input_i)

        return inputs

    def _get_master_inputs(self, batch, transition_idx, slave_hidden_state):
        """
        计算所有集群master的输入。
        batch为所有集群的输入，计算时需要对集群进行区分，取出对应的样本信息进行计算。
        batch维度：(batch_size, max_episode_len, n_agents, shape)
        slave_hidden_state为一个列表，每个元素维度：(batch_size * n_slave, slave_hidden_dim + attention_dim)
        """
        s = batch["s"][:, transition_idx]
        # batch中s为(batch_size, max_episode_len, state_shape)，此函数中的s为从第二维度取出某一个时间步后，故不是三维
        # s是二维没有n_agents维度，因为所有agent的s一样，其他数据都是三维不能拼接，所以要把s转化为三维: (batch_size, n_master, state_shape)
        s = s.unsqueeze(1).expand(-1, self.n_master, -1)

        if self.args.cuda:
            s = s.cuda()
            for i in range(len(slave_hidden_state)):
                slave_hidden_state[i] = slave_hidden_state[i].cuda()

        '''
        master的输入除了全局观测外，还有自己组内slave的隐藏状态
        s的维度是(batch_size, n_master, state_shape)
        '''
        return s, slave_hidden_state
