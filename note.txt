V1.2：
1.加入HAMS算法
2.reward增大
3.batch_size调至32
4.环境reward设置进行调整，在终局获胜时若master存活则额外加入一个存活奖励

V1.3：
1.HAMS算法的master变为虚拟的，所有集群内的智能体都是slave
2.删除终局master存活的奖励

V1.4：
1.将msmarl的master变为虚拟的，即slave数量就是智能体数量

V1.5：
1.HAMS在采样时保存对应动作的概率和可选动作概率，即将动作概率纳入样本，训练网络时不再执行前向计算概率
2.HAMS写好Optimal Baseline函数，但未接入算法
3.每轮最后一个epoch结束后，重新测试并画上最后一个点
4.runner.py的plt函数加上close

V1.6：
1.HAMS前向过程合并为一个网络，输出分布和v值
2.改成基于PPO方式更新（单线程）
3.使用GAE计算折扣回报

V1.6-half-test：
1.加入多进程，实现多尺度交叉熵采样